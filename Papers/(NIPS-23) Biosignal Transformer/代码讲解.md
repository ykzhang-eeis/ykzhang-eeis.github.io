# BIOT代码模块讲解

**本文将从以下几个方面来讲解BIOT(bio-signal Transformer)的代码**

1. 网络模块是如何搭建的（画思维导图）？
2. 原始数据的输入格式是什么？如何处理数据不等长的问题？
3. Dataloader过程怎么写的，输入到网络中的shape是什么？
4. Transformer的哪一步骤建立了时序关联性？
5. 对于无监督学习而言，正负样本对的构建形式是怎么样的？
6. 无监督学习的目标损失函数是什么？
7. 网络的性能评价指标是什么？

---

- BIOT网络的模块结构导图

![image-20231027143445997](../../images/image-20231027143445997.png)

可以看到几个基础模块分别为 `PatchFrequencyEmbedding` , `LinearAttentionTransformer`, `PositionalEncoding`, `Classification`和`prediction`

要关注的一些网络参数

1. `emb_size`：嵌入向量的维度，表示模型在内部使用的嵌入维度。
2. `n_freq`：频域的频率分量数量，通常与傅里叶变换的参数相关。
3. `n_classes`：任务中的类别数量，表示模型的输出类别数量。
4. `d_model`：模型的维度，通常表示模型内部处理的特征维度，例如注意力头的维度。
5. `max_len`：输入序列的最大长度，通常用于位置编码。
6. `heads`：多头自注意力机制中的头数，用于增加模型的表示能力。
7. `depth`：模型的深度，表示模型中堆叠的多头自注意力层的数量。
8. `n_fft`：傅里叶变换中的窗口大小，用于从时域转换到频域。
9. `hop_length`：傅里叶变换中窗口之间的跳跃长度，通常与信号的时间分辨率有关。

```python
class PatchFrequencyEmbedding(nn.Module):
    def __init__(self, emb_size=256, n_freq=101):
        super().__init__()
        self.projection = nn.Linear(n_freq, emb_size)
    
    def forward(self, x):
        """
            x: (batch, 1, n_freq, time)
            out: (batch, time, emb_size)
        """
        # (batch, 1, n_freq, time) -> (batch, n_freq, time) -> (batch, time, n_freq)
        x = x.squeeze(1).permute(0, 2, 1) 
        # (batch, time, n_freq) -> (batch, time, emb_size)
        output = self.projection(x)
        return output
```

```python
class ClassificationHead(nn.Module):
    def __init__(self, emb_size, n_classes):
        super().__init__()
        self.clshead = nn.Sequential(
            nn.ELU(),
            nn.Linear(emb_size, n_classes)
        )
    
    def forward(self, x):
        """
        	x: (batch, emb_size)
        	out: (batch, n_classes)
        """
        out = self.clshead(x)
        return out
```

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float=0.1, max_len: int = 1000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model, dtype=torch.float)
        pe.requires_grad = False

        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0) # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        	x: (batch, time, emb_size)
        	self.pe[:, :x.size(1)]: (1, time, emb_size)
        """
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

```python
class BIOTEncoder(nn.Module):
    def __init__(self, emb_size=256, heads=8, depth=4, n_channels=16, n_fft=200, hop_length=100, **kwargs):
        super().__init__()
        self.n_fft = n_fft # 傅里叶变换的窗口大小
        self.hop_length = hop_length # 窗口之间的跳跃长度

        self.patch_embedding = PatchFrequencyEmbedding(emb_size=emb_size, n_freq=n_fft // 2 + 1)
        self.transformer = LinearAttentionTransformer(
            dim = emb_size,
            heads = heads,
            depth = depth,
            max_seq_len = 1024,
            attn_layer_dropout = 0.2,
            attn_dropout = 0.2,
        )
        self.positional_encoding = PositionalEncoding(d_model=emb_size)

        # num_embeddings 表示要映射的类别或整数的总数，embedding_dim 表示每个嵌入向量的维度，将每个通道映射到一个256维的向量
        self.channel_tokens = nn.Embedding(num_embeddings=n_channels, embedding_dim=256) 
        # self.index是表示每个通道的索引，确保不同通道的顺序是固定的，以便在模型中正确处理每个通道的数据。
        self.index = nn.Parameter(torch.LongTensor(range(n_channels)), requires_grad=False)

    def stft(self, sample):
        # sample.shape: (batch, num_channels, time)
        signal = []
        for s in range(sample.shape[1]):
            spectral = torch.stft(
                input = sample[:, s, :],
                n_fft = self.n_fft,
                hop_length = self.hop_length,
                center = False, # 窗口是否居中
                onesided = True, # 是否只保留单侧频谱
                return_complex = True, # 返回的是否是复数形式的频谱
            )
            signal.append(spectral)
        stacked = torch.stack(signal).permute(1, 0, 2, 3)
        return torch.abs(stacked)
    
    def forward(self, x, n_channel_offset=0, perturb=False):
        """
            x: (batch, channel, time)
            output: (batch, emb_size)
        """
        emb_seq = []
        for i in range(x.shape[1]):
            channel_spec_emb = self.stft(x[:, i:i+1, :])
            channel_spec_emb = self.patch_embedding(channel_spec_emb) # (batch, time, emb_size)
            batch_size, ts, _ = channel_spec_emb.shape  
            # (batch, time, emb_size)
            channel_token_emb = (self.channel_tokens(self.index[i + n_channel_offset]).unsqueeze(0).unsqueeze(0).repeat(batch_size, ts, 1))
            channel_emb = self.positional_encoding(channel_spec_emb + channel_token_emb)

            if perturb:
                ts_new = np.random.randint(ts//2, ts)
                selected_ts = np.random.choice(range(ts), ts_new, replace=False)
                channel_emb = channel_emb[:, selected_ts]
            emb_seq.append(channel_emb)
        
        # (batch, channel * ts, emb_size)
        emb = torch.cat(emb_seq, dim=1)
        # (batch, emb_size)
        emb = self.transformer(emb).mean(dim=1)
        return emb
```

