# ANOMALY TRANSFORMER: TIME SERIES ANOMALY DETECTION WITH ASSOCIATION DISCREPANCY

## 无监督时间序列异常检测

### Abstract

时间序列中无监督检测异常点需要模型得出一个可区分的标准。先前的方法主要通过学习逐点表示或成对关联来解决这个问题，然而，这两者都不足以推断复杂的动态关系

Transformers在统一建模逐点表示和成对关联方面表现出强大的能力，每个时间点的自注意权重分布可以充分体现与整个序列的丰富关联

异常点是罕见的，所以从异常点建立到整个序列的非平凡的关联是困难的，所以异常点的关联主要集中在他们相邻的时间点

这种相邻集中偏差意味着一个基于关联的标准在正常点和异常点之间天然具有区分性，我们通过"关联差异"来强调这一点。从技术上讲，我们提出了一种具有新的异常-关注机制的异常transformer结构来计算关联差异。采用极小极大策略来增强关联差异的正常-异常区分能力

### Introduction

一种自然且实际的异常标准是逐点重建或预测误差。然而，由于异常的罕见性，逐点表示对于复杂的时间模式来说信息较少，并且可能会被正常时间点主导，从而使异常难以区分。此外，逐点重建或预测误差是逐点计算的，不能提供对时间上下文的全面描述

基于子序列之间的相似性来检测异常，虽然考虑了时间特性，但是也不能捕捉每一个时间点和整个序列之间的细粒度时间关联

Transformer具有在全局表示和长程关系的统一建模方面的强大能力。将Transformer应用于时间序列，发现每个时间点的时间关联可以从自注意图中获取，这表现为在时间维度上，所有时间点的关联权重分布

相较于异常模式的罕见性，正常时间点更易于发现与整个序列相关的信息，而不仅仅局限于相邻区域，基于这一观察，我们尝试利用关联分布的天然正常-异常区分能力。这导致了每个时间点的新异常标准，由每个时间点的先验关联和其系列关联之间的距离来量化，称为“关联差异”。如前所述，由于异常的关联更有可能是相邻集中的，因此异常将呈现出比正常时间点更小的关联差异。

我们将Transformer引入无监督时间序列异常检测，并提出了Anomaly Transformer来进行关联学习。为了计算关联差异，我们**将自注意机制改进为Anomaly-Attention**，它**包含一个双分支结构**，分别用于建模每个时间点的**先验关联**和**系列关联**。先验关联采用可学习的高斯核来呈现每个时间点的相邻集中归纳偏差，而系列关联对应于从原始序列中学习的自注意权重。此外，在两个分支之间应用了一个极小极大策略，可以增强关联差异的正常-异常区分能力，并进一步得出新的基于关联的标准

### Method

无监督时间序列异常检测的关键在于学习信息丰富的表示并找到可区分的标准，假设观察到的时间序列$\chi$ 由一组时间点表示，即$\{x_1,x_2,\cdots,x_N\}$，其中在$t$时刻的观测值$x_t\in\mathbb{R}^d$，$d$表示数据维数，$L$表示层数量，每一层的特点是交替堆叠 Anomaly-Attention 块和 Feed-Forward 层，这种堆叠结构有助于从深层多层特征中学习潜在关联

![image-20231021173816052](C:\Users\zyk\Desktop\BaiduSyncdisk\Github_local_repo\ykzhang-eeis.github.io\images\image-20231021173816052.png)

Anomaly-Attention（左侧）同时建模了先验关联和系列关联。除了重建损失，我们的模型还通过特别设计的停梯度机制（灰色箭头）进行极小极大策略的优化，以限制先验关联和系列关联，使关联差异更具区分性。

- 单分支的自注意机制不能同时建模先验关联和系列关联。我们提出了具有双分支结构的Anomaly-Attention 。
  - 对于先验关联，我们**采用可学习的高斯核来计算相对时间距离的先验**。由于高斯核的单峰性质，这一设计可以更注重相邻时间构造。我们还使用可学习的尺度参数σ来调整高斯核，使先验关联适应各种时间序列模式，如不同长度的异常段
  - 对于**系列关联**，该分支用于从原始时间序列中学习关联，可以自适应地找到最有效的关联

对于第$l$个 Anomaly-Attention 层来说，有：
$$
\text{Initialization: }\mathcal{Q},\mathcal{K},\mathcal{V},\sigma=\mathcal{X}^{l-1}W_{\mathcal{Q}}^{l},\mathcal{X}^{l-1}W_{\mathcal{K}}^{l},\mathcal{X}^{l-1}W_{\mathcal{V}}^{l},\mathcal{X}^{l-1}W_{\sigma}^{l}
$$

$$
\text{Prior-Association: }\mathcal{P}^l=\text{Rescale}\left(\left[\frac{1}{\sqrt{2\pi}\sigma_i}\exp\left(-\frac{|j-i|^2}{2\sigma_i^2}\right)\right]_{i,j\in\{1,\cdots,N\}}\right)
$$

$$
\text{Series-Association:}\mathcal{S}^l=\text{Softmax}\left(\frac{\mathcal{QK}^\mathrm{T}}{\sqrt{d_{\mathrm{model}}}}\right)
$$

$$
\text{Reconstruction: }\widehat{\mathcal{Z}}^l=\mathcal{S}^l\mathcal{V}
$$

- the pseudo-code of Anomaly-Attention

![image-20231021180150242](C:\Users\zyk\Desktop\BaiduSyncdisk\Github_local_repo\ykzhang-eeis.github.io\images\image-20231021180150242.png)

本文的创新结构代码（加了一个高斯核）

```python
class AnomalyAttention(nn.Module):
    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):
        super().__init__()
        self.mask_flag = mask_flag
        self.scale = scale
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)
        self.distance = torch.zeros((win_size, win_size)).cuda()
        for i in range(win_size):
            for j in range(win_size):
                self.distance[i][j] = abs(i-j)

    def forward(self, queries, keys, values, sigma, attn_mask):
        batch_size, seq_len, heads, dim = queries.shape
        scale = self.scale or 1. / math.sqrt(dim)

        scores = torch.einsum("blhd,bshd->bhls", queries, keys)
        if self.mask_flag:
            if attn_mask is None:
                attn_mask = TriangularCausalMask(batch_size, seq_len, device=queries.device)
            scores.masked_fill_(attn_mask, value=-np.inf) # in-place operation
        attn_score = scale * scores # (batch_size, heads, seq_len, seq_len)

        sigma = sigma.transpose(1, 2) # (batch_size, seq_len, heads) -> (batch_size, heads, seq_len)
        window_size = attn_score.shape[-1]
        sigma = torch.sigmoid(sigma * 5) + 1e-5
        sigma = torch.pow(3, sigma) - 1 # (batch_size, heads, seq_len)
        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size) # (batch_size, heads, seq_len, seq_len), 前三个维度不变，只在最后一个维度上重复seq_len次
        prior = self.distance.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda() # (batch_size, heads, seq_len, seq_len)
        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))

        attn = self.dropout(torch.softmax(attn_score, dim=-1)) # (batch_size, heads, seq_len, seq_len)
        output = torch.einsum("bhls,bshd->blhd", attn, values) # (batch_size,, seq_len, heads, dim)

        if self.output_attention:
            return output.contiguous(), attn, prior, sigma
        else:
            return output.contiguous(), None
```

